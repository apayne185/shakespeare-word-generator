# Shakespearean Text Generator using N-Gram Model & NLTK
#### By Anna Payne

This code generates Shakespearea-like text using an n-gram model (2-gram, 3-gram, 4-gram) trained on Shakespeare data (his plays) from the NLTK Corpus. The model utilizes preprocessing of the Shakespeare data, resuable code of n-gram counting, probability calculations, token sampling, and iterative generation of the text.

### The following tasks are accomplished:
* **Data Preprocessing**: The text is loaded from NLTKs Shakespeare corpus, then converted to lowercase/punctuation is removed, and split into tokens (each word = token).
* **N-gram Model**: The n-grams are created, and transition probabilities from one token to the next are calculated for bigrams, trigrams, quadgrams.
* **Text Generation**: Text is generated by selecting the next word based on n-gram probabilities, iteratively starting from an initial n-gram.
* **Exploration of N-grams**: The model supports bigrams, trigrams, and quadgrams with a comparison of generated text quality.


### Function Breakdown - By File
#### src/ngram_model.py
##### `preprocess()`:
   * Loads Shakespeare data using NLTK corpus.
   * Converts text to lowercase & removes punctuation.
   * The splits the text into a list of tokens.

##### `create_ngrams(tokens, n)`:
   * Inputs of tokens (list) and n (int), which is the size of the ngram. 
   * Creates n-grams (tuples of 2,3,4) from the list of tokens.
   * If number of tokens is less then n, return an empty list because it means no ngrams can be made. 

##### `from_ngram_to_next_token_counts(ngrams)`:
   * Inputs of ngrams (list)
   * Outputs a dictionary where keys are n-grams (prefix = all words but the last (the last word is considered the next word)), and values are dictionaries of the counts of following tokens.
   * Counts occurance of words following each ngram. 

##### `from_ngram_to_next_token_probs(ngram_counts)`:
   * Takes an input of ngram_counts (dict) which is the frequency counts. 
   * Then converts frequency counts into probabilities by diving its count by the total.
   * Outputs a nested dictionary og the ngrams. 

##### `sample_next_token(ngram, ngram_prob)`:
   * Randomly samples a next token/word based on the probability distribution of a given ngram. 
   * Retrieves probability distribution for an ngram, if no ngram is found, tries up to 3 random ngrams from ngram_prob. 


#### src/text_generation.py
##### `generate_text_from_ngram(initial_ngram, length, ngram_probs, n)`:
   * Generates text starting with an initial n-gram (prefix).
   * Iteratively generates the next token based on probabilities until the desired length is reached.
   * Essentially is generating the sequence of words by predicting the next word using ngram_prob & the sample_next_token function from the ngram_model.py. Also ensures no repeating words are included. 


#### main.py
##### `main()`:
   * The main driver function that runs the program.
   * Takes user input (int: 2,3,4) for the n-gram size and calls the function to generates text using the trained n-gram model.
   * Calls the functions preprocess() and exits if no tokens exist.
   * Generates the ngram list from the tokens, if not enough tokens exist, exits the program. 
   * Calls from_ngram_to_next_token_counts() and from_ngram_to_next_token_probs() to create a dictionary of ngrams and next word counts, then converts counts to probabilties. If no probabilities exist, exits. 
   * Randomly selects a starting ngram and generates 50 words from it. 


#### tests/testcases.py 
Uses pytest to use unit testing for the functions in ngram_model.py and text_generation.py. 

##### `test_preprocess()`: 
   * Ensures preprocess returns a non-empty list with all string elements. 
   
##### `test_create_ngrams()`: 
   * Ensures create_ngrams returns a list of tuples. 

##### `test_from_ngram_to_next_token_counts()`: 
   * Ensures from_ngram_to_next_token_counts returns a dictionary and each value inside is also a dictionary. 

##### `test_from_ngram_to_next_token_probs()`: 
   * Ensures from_ngram_to_next_token_probs returns a dictionary, that each value inside is also a dictionary, and that each probability is between 0-1.

##### `test_generate_text_from_ngram()`: 
   * Ensures generate_text_from_ngram returns a string and that the generated text contains 50 words. 

 


### User Feedback of Generated Text
User feedback was conducted via Google Forms. The questions asked were:
* What is your familiarity with Shakespearean English?
* What is your background? 
* Does the gen. text sound Shakespearean?  
* How natural does this text feel?
* Does this text capture Shakespeareâ€™s poetic rhythm and style?  
* How coherent is this text? (Does it make sense overall?)
* & Other comments :


Based on the data gathered, here are some observations made: 
* Most of the participants were students with limited Shakespeare exposure. 
* The generated text sounded "Somewhat" / "Yes" Shakespearian. The users felt that the text captured his style and represented it well in the outputted text. 
* The text felt "Somewhat unnatural" and "Neutral" when users were asked about how natural it felt. Most users felt that the text was slightly confusing and robotic. 
* The text "slightly" captured his poetic rhythm and style. 
* The text is also "Somewhat coherent" when users rated it. 
* The other comments provided about the generated text reflected these results. It was mentioned how the text did capture his vocabulary and style and moderately followed basic sentence and grammar structure. However, the meaning of the sentences was not easy to follow and did not capture any real meaning. 

#### Next Steps
Any next steps would be to start fine-tuning the meaning/direction of the text generated considering more data and details. Continuing to expand to larger ngrams would also assist with the performance. 

   


   







